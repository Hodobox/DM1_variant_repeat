A parsable list of attempted optimizations (resp. changes)
(compared to original athttps://github.com/picrin/science_of_alignment/blob/master/variant_repeat_algorithm.ipynb)

General:

- Switch from Python to C++

- produce_sequences eliminates duplicates

- produce_seq_improved - instead of merging results (which requires passing around vectors of generated sequences)
we pass an outside container by reference and add them instead. Shortens pattern generation to ~1s. Does not shorten templates down to sequence size (to achieve
unambiguous template-parameter relationships)

Generic align function:

- no longer returns the string which best matched the given state.
We know the sequence/template arguments for the top-level align call
and can score the corret template accordingly. Internal information is unnecessary

- we don't slice the strings within align, we just remember where each string is
supposed to end in the current state and work with that character

- instead of a (sequence, template) : result mapping, we utilize the fact that 
(as all characters are removed from the right) the lengths of the sequence and 
template by itself uniquely identify the state
of the DP, and so a [sequence size+1][template size+1] integer array can be used as 
the cache instead (with value being
best found alignment score)

- instead of sorting the three results, a simple max() is way faster

- all functions except original are multithreading-compatible

Specific align functions:

	align_original:
		simply the original python rewritten to C++ along with all 'generic' optimisations

	align_greedymatch_multithread:
		if current character matches, return match result without even trying deletion or gap
	 	reasoning:
		let the strings be Tc Sc (some prefixes T,S and ending with the same character c)
		can a higher alignment score be achieved by choosing a gap or delete in this step as opposed to match?
		if we choose some number of gaps (i.e. gap in this step and 0-or-more gaps right after),
		followed by a deletion then whatever alignment score is reached thanks to that, 
		we could improve it by appending the match from this step to it
		(some number of deletions followed by a gap analogous)
		if we instead choose some number of gaps/deletions followed by a match, we can just as well match the current character  
		and gap/delete the one matched in the gaps/deletions call 
		i.e. if from (TcXc, Sc) we do some gaps to get to (Tc,Sc) and then match c to get (T,S),
		we can instead match c immediately to get (TcX,S), then do the same number of gaps as in the above case to get (T,S)
		with the same score.

		this optimisation does not alter any scores compared to original in a randomized 20-vs-20 test
		(test: align_original_greedy_match.txt)

	align_GAL_multithread:
		we note that a significant number of searched states will be pointless, in that we obtain them by only making
		terrible decisions (e.g. choosing gap every other step) and the computed values will be discarded in opposition to
		solutions which went a 'sensible' route. We attempt to prune such branches from our recursive search.

		Each align call has a get_at_least parameter, with the following meaning: the result you compute will only matter
		to the caller if it will be at least get_at_least. As such, if even in the best case the score will be less,
		don't bother with the search and just return a failure.

		This parameter is passed to each of the three recursive calls (- the score that step provides, 
		e.g. if get_at_least = 10 and we do a gap worth -1 points, the recursive call will need to get at least a score of 11),
		and updated on the fly (e.g. if get_at_least = 10 but the match recursive call returns a score of 20,
		the delete call will be given 20, similarly if that improves it yet again, gap gets that value)

		The top-level call sets this parameter to a terrible, negative value, so only actually legitimate get_at_least values
		that were computed at some point will be used for pruning.

		Unfortunately, even though we know that (on real data where sequences have length > 1000) we expect close matches with score
		of 700-900 (have never seen anything under 500), we cannot initialize the top-level call with a larger get_at_least value
		for the following reason: if a recursive branch makes bad decisions, in such a way that we end up in a position where none
		of the three options give us the get_at_least value (originaly equal to some lower threshold, e.g. 300), the cached result 
		for this state will be this faulty value composed of the three branches all returning failures 
		(maybe from their own, nested calls). If a more 'sensible' call then reaches the same state (i.e. it made 'sensible'
		decisions and has achieved most of its get_at_least value already, so its now only = 10), it will return this invalid,
		terrible score cached by the previous 'bad decision maker' which gave up without actually aligning 
		the rest of the sequence, and so this propagates upwards 
		(usually from multiple branches of a high-level call), significantly
		lowering the eventual best alignment score found.

		The original optimization is however imperfect, 
		and Hodobox doesn't know why the following test results are what they are:

		setting the get-at-least value to an unrealistically high value 
		(e.g. assuming the shorter string is matched perfectly and the rest
		is discarded) sometimes gives results of ~1-2 points less.

		setting it to a realistic value (shorter is matched, rest is gapped/deleted)
		very often gives results of ~1-2 points less.

	align_GAL_gm:
		adding the greedy-match optimisation described above on top, decreases some of 
		the results by ~1-2 points yet again.

	align_gm_mt_opt:
		stands for align_greedymatch_multithread_optimized.
		WIP, will try and precalculate results for prefixes and suffixes first.


deprecated:

	align_get_at_least_short:
		16bit integers are enough for all values, so tried if replacing ints by int16s will be an improvement 
		(perhaps with caching).
		Nope.
		optimisation deprecated.

	align_get_at_least_array:
		instead of creating a brand-new  
		vector<vector<int> > cache(candidate_patterns[i].size()+1,vector<int>(sequences[k].size()+1,UNCACHED))
		for each (template,sequence) pair, a single global 2D array cached[1501][1501] is used.
		for fast clearing between pairs, a 2D bool array is used to keep track of which states have been memoised,
		and is memset before each top-level align. Surprisingly, there is a ~10% improvement compared to the vector version.
		
		deprecated after multithreading was brought in